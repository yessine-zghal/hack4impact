{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Oj_-BCwqSpvx"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import glob\n",
    "import cv2\n",
    "import math\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "AhM8XTWDS1_6",
    "outputId": "f84696c2-a0ab-4171-df2a-c96e1957e108"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xaMtlzZDS5x5"
   },
   "outputs": [],
   "source": [
    "allimg_train = []\n",
    "alllbl_train = []\n",
    "allimg_test = []\n",
    "alllbl_test = []\n",
    "i=0\n",
    "for folder in os.listdir('/content/drive/MyDrive/pcd/pythonProject/dataset'):\n",
    "  imgL=[]\n",
    "  lblL=[]\n",
    "  i=i+1\n",
    "  for img in glob.glob(\"/content/drive/MyDrive/pcd/pythonProject/dataset/\"+folder+\"/*.jpg\"):\n",
    "    n = cv2.imread(img)\n",
    "    n = cv2.resize(n, (64,64), interpolation = cv2.INTER_AREA)\n",
    "    imgL.append(n)\n",
    "    lblL.append(i)\n",
    "  allimg_train.extend(imgL[:int(0.9*len(imgL))])\n",
    "  allimg_test.extend(imgL[int(0.9*len(imgL)):-1])\n",
    "  alllbl_train.extend(lblL[:int(0.9*len(imgL))])\n",
    "  alllbl_test.extend(lblL[int(0.9*len(imgL)):-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "UoXQkSqWg3sz",
    "outputId": "742ffd30-4d13-4155-fa94-a5fb1ffa10f8"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "38492"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(alllbl_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "c0KetUVv0Y2y"
   },
   "outputs": [],
   "source": [
    "for i in range(len(alllbl_train)):\n",
    "  alllbl_train[i]=alllbl_train[i]-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ad05Ll3CrVp_"
   },
   "outputs": [],
   "source": [
    "for i in range(len(alllbl_test)):\n",
    "  alllbl_test[i]=alllbl_test[i]-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nao3tWz8esUs"
   },
   "outputs": [],
   "source": [
    "allimg_train = tf.constant(allimg_train)\n",
    "alllbl_train = tf.constant(alllbl_train)\n",
    "allimg_test = tf.constant(allimg_test)\n",
    "alllbl_test = tf.constant(alllbl_test)\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((allimg_train , alllbl_train))\n",
    "test_dataset = tf.data.Dataset.from_tensor_slices((allimg_test, alllbl_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "db4dbO8cbCQe",
    "outputId": "ba291762-1abc-4317-9b69-f6e5dcc33099"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4261"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(allimg_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "75i-1TOCWHlY"
   },
   "outputs": [],
   "source": [
    "def normalize(images, labels):\n",
    "  \n",
    "  images = tf.cast(images, tf.float32)\n",
    "  images /= 255\n",
    "  return images, labels\n",
    "\n",
    "\n",
    "train_dataset =  train_dataset.map(normalize)\n",
    "test_dataset  =  test_dataset.map(normalize)\n",
    "\n",
    "\n",
    "train_dataset =  train_dataset.cache()\n",
    "test_dataset  =  test_dataset.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RFVvPICkWdA4"
   },
   "outputs": [],
   "source": [
    "BATCH_SIZE = 20\n",
    "num_train_examples = len(allimg_train)\n",
    "train_dataset = train_dataset.cache().repeat().shuffle(num_train_examples).batch(BATCH_SIZE)\n",
    "test_dataset = test_dataset.cache().batch(BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "f6HSTJQ8Wfqf",
    "outputId": "cdfb3598-4de5-4980-8196-1a81e9dcbfc8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d (Conv2D)             (None, 64, 64, 128)       3584      \n",
      "                                                                 \n",
      " max_pooling2d (MaxPooling2D  (None, 21, 21, 128)      0         \n",
      " )                                                               \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 21, 21, 128)       0         \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 21, 21, 128)       147584    \n",
      "                                                                 \n",
      " max_pooling2d_1 (MaxPooling  (None, 7, 7, 128)        0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " conv2d_2 (Conv2D)           (None, 7, 7, 64)          73792     \n",
      "                                                                 \n",
      " max_pooling2d_2 (MaxPooling  (None, 2, 2, 64)         0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " conv2d_3 (Conv2D)           (None, 2, 2, 64)          16448     \n",
      "                                                                 \n",
      " max_pooling2d_3 (MaxPooling  (None, 1, 1, 64)         0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 1, 1, 64)          0         \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 64)                0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 256)               16640     \n",
      "                                                                 \n",
      " dropout_2 (Dropout)         (None, 256)               0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 32)                8224      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 266,272\n",
      "Trainable params: 266,272\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model2 = tf.keras.Sequential()\n",
    "model2.add(tf.keras.layers.Conv2D(filters=128, kernel_size=3, padding='same', activation='relu', input_shape=(64,64,3))) \n",
    "model2.add(tf.keras.layers.MaxPooling2D(pool_size=3))\n",
    "model2.add(tf.keras.layers.Dropout(0.3))\n",
    "model2.add(tf.keras.layers.Conv2D(filters=128, kernel_size=3, padding='same', activation='relu'))\n",
    "model2.add(tf.keras.layers.MaxPooling2D(pool_size=3))\n",
    "model2.add(tf.keras.layers.Conv2D(filters=64, kernel_size=3, padding='same', activation='relu'))\n",
    "model2.add(tf.keras.layers.MaxPooling2D(pool_size=3))\n",
    "model2.add(tf.keras.layers.Conv2D(filters=64, kernel_size=2, padding='same', activation='relu'))\n",
    "model2.add(tf.keras.layers.MaxPooling2D(pool_size=2))\n",
    "model2.add(tf.keras.layers.Dropout(0.3))\n",
    "model2.add(tf.keras.layers.Flatten())\n",
    "model2.add(tf.keras.layers.Dense(256, activation='relu'))\n",
    "model2.add(tf.keras.layers.Dropout(0.5))                \n",
    "model2.add(tf.keras.layers.Dense(32, activation='softmax'))\n",
    "\n",
    "model2.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PjDfl9TxWusQ"
   },
   "outputs": [],
   "source": [
    "model2.compile(optimizer='adam',\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KnkRRWAsCrat"
   },
   "outputs": [],
   "source": [
    "from keras.callbacks import ReduceLROnPlateau\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=1, min_lr=0.0005)\n",
    "early_stop = EarlyStopping(monitor='val_loss', min_delta=0, patience=2, verbose=0, mode='auto')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "B0DHNBLhW1Af",
    "outputId": "acf0725a-74fe-40c9-ec08-43d9ac0eb27c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "3849/3850 [============================>.] - ETA: 0s - loss: 0.2441 - accuracy: 0.9322WARNING:tensorflow:Learning rate reduction is conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy,lr\n",
      "WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy,lr\n",
      "3850/3850 [==============================] - 46s 12ms/step - loss: 0.2441 - accuracy: 0.9323 - lr: 0.0010\n",
      "Epoch 2/10\n",
      "3848/3850 [============================>.] - ETA: 0s - loss: 0.2464 - accuracy: 0.9331WARNING:tensorflow:Learning rate reduction is conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy,lr\n",
      "WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy,lr\n",
      "3850/3850 [==============================] - 45s 12ms/step - loss: 0.2463 - accuracy: 0.9331 - lr: 0.0010\n",
      "Epoch 3/10\n",
      "3847/3850 [============================>.] - ETA: 0s - loss: 0.2390 - accuracy: 0.9349WARNING:tensorflow:Learning rate reduction is conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy,lr\n",
      "WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy,lr\n",
      "3850/3850 [==============================] - 44s 11ms/step - loss: 0.2389 - accuracy: 0.9350 - lr: 0.0010\n",
      "Epoch 4/10\n",
      "3847/3850 [============================>.] - ETA: 0s - loss: 0.2505 - accuracy: 0.9319WARNING:tensorflow:Learning rate reduction is conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy,lr\n",
      "WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy,lr\n",
      "3850/3850 [==============================] - 44s 11ms/step - loss: 0.2504 - accuracy: 0.9319 - lr: 0.0010\n",
      "Epoch 5/10\n",
      "3848/3850 [============================>.] - ETA: 0s - loss: 0.2356 - accuracy: 0.9365WARNING:tensorflow:Learning rate reduction is conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy,lr\n",
      "WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy,lr\n",
      "3850/3850 [==============================] - 44s 11ms/step - loss: 0.2356 - accuracy: 0.9365 - lr: 0.0010\n",
      "Epoch 6/10\n",
      "3848/3850 [============================>.] - ETA: 0s - loss: 0.2471 - accuracy: 0.9341WARNING:tensorflow:Learning rate reduction is conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy,lr\n",
      "WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy,lr\n",
      "3850/3850 [==============================] - 44s 11ms/step - loss: 0.2471 - accuracy: 0.9341 - lr: 0.0010\n",
      "Epoch 7/10\n",
      "3850/3850 [==============================] - ETA: 0s - loss: 0.2389 - accuracy: 0.9371WARNING:tensorflow:Learning rate reduction is conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy,lr\n",
      "WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy,lr\n",
      "3850/3850 [==============================] - 44s 11ms/step - loss: 0.2389 - accuracy: 0.9371 - lr: 0.0010\n",
      "Epoch 8/10\n",
      "3846/3850 [============================>.] - ETA: 0s - loss: 0.2469 - accuracy: 0.9353WARNING:tensorflow:Learning rate reduction is conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy,lr\n",
      "WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy,lr\n",
      "3850/3850 [==============================] - 44s 11ms/step - loss: 0.2468 - accuracy: 0.9353 - lr: 0.0010\n",
      "Epoch 9/10\n",
      "3846/3850 [============================>.] - ETA: 0s - loss: 0.2511 - accuracy: 0.9338WARNING:tensorflow:Learning rate reduction is conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy,lr\n",
      "WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy,lr\n",
      "3850/3850 [==============================] - 44s 11ms/step - loss: 0.2509 - accuracy: 0.9338 - lr: 0.0010\n",
      "Epoch 10/10\n",
      "3850/3850 [==============================] - ETA: 0s - loss: 0.2485 - accuracy: 0.9381WARNING:tensorflow:Learning rate reduction is conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy,lr\n",
      "WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy,lr\n",
      "3850/3850 [==============================] - 44s 11ms/step - loss: 0.2485 - accuracy: 0.9381 - lr: 0.0010\n"
     ]
    }
   ],
   "source": [
    "\n",
    "history=model2.fit(train_dataset, epochs=10, steps_per_epoch=math.ceil(num_train_examples/BATCH_SIZE),callbacks=[reduce_lr, early_stop])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ASu0CUM-i3-H",
    "outputId": "87964cf7-43ee-4224-a53a-1840f7359c1b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /content/drive/MyDrive/pcd/pythonProject/alphabetcha1.model/assets\n"
     ]
    }
   ],
   "source": [
    " model2.save('/content/drive/MyDrive/pcd/pythonProject/alphabetcha1.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 252
    },
    "id": "u939dsezkAPe",
    "outputId": "932bb383-368e-498e-c144-a826207a1697"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hi\n"
     ]
    },
    {
     "ename": "error",
     "evalue": "ignored",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31merror\u001b[0m                                     Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-32-5db243cb269b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m     \u001b[0mret\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mframe\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcam\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m     \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'eyaf.jpg'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mframe\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     38\u001b[0m     \u001b[0mframe\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'snapshot.jpg'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31merror\u001b[0m: OpenCV(4.1.2) /io/opencv/modules/imgcodecs/src/loadsave.cpp:715: error: (-215:Assertion failed) !_img.empty() in function 'imwrite'\n"
     ]
    }
   ],
   "source": [
    "# from tensorflow import keras\n",
    "model = tf.keras.models.load_model(r\"/content/alphabetcha.model\")\n",
    "print('hi')\n",
    "background = None\n",
    "accumulated_weight = 0.5\n",
    "ROI_top = 100\n",
    "ROI_bottom = 300\n",
    "ROI_right = 150\n",
    "ROI_left = 350\n",
    "\n",
    "word_dict = {0: 'ain', 1: 'al', 2:'aleff',3: 'bb', 4: 'dal',5:'dha', 6: 'dhad', 7:'fa',8:'gaaf',9: 'ghain', 10: 'ha', 11:'haa',12: 'jeem', 13: 'kaaf',\n",
    "             14:'khaa',15: 'la',16:'laam',17:'meem',18:'nun',19:'ra', 20:'saad',21:'seen',22:'sheen',23:'ta',24:'taa',25:'thaa',26:' thal',27:'toot',28:'waw',29:'ya',30:'yaa',31:'zay'}\n",
    "def cal_accum_avg(frame, accumulated_weight):#detection de background\n",
    "\n",
    "    global background\n",
    "\n",
    "    if background is None:\n",
    "        background = frame.copy().astype(\"float\")\n",
    "        return None\n",
    "    cv2.accumulateWeighted(frame, background, accumulated_weight)\n",
    "\n",
    "def segment_hand(frame, threshold=25):#segment our hand using contour detection\n",
    "        global background\n",
    "        a, thresholded = cv2.threshold(cv2.absdiff(background.astype(\"uint8\"), frame), threshold, 255, cv2.THRESH_BINARY)\n",
    "        contours, hierarchy =cv2.findContours(thresholded.copy(), cv2.RETR_EXTERNAL,cv2.CHAIN_APPROX_SIMPLE)\n",
    "        if len(contours) == 0:\n",
    "             return None\n",
    "        else: \n",
    "            hand_segment_max_cont = max(contours, key=cv2.contourArea)\n",
    "            return (thresholded, hand_segment_max_cont) \n",
    "\n",
    "\n",
    "cam = cv2.VideoCapture(0)\n",
    "num_frames = 0\n",
    "while True:\n",
    "    ret, frame = cam.read()\n",
    "    cv2.imwrite('eyaf.jpg', frame)\n",
    "    frame = cv2.imread('snapshot.jpg',0)\n",
    "\n",
    "    frame = cv2.flip(frame, 1)    #flipping the frame to prevent inverted image of captured frame...\n",
    "    frame_copy = frame.copy()\n",
    "\n",
    "    roi = frame[ROI_top:ROI_bottom, ROI_right:ROI_left]\n",
    "\n",
    "    gray_frame = cv2.GaussianBlur(frame, (9, 9), 3)\n",
    "\n",
    "    if num_frames < 40:\n",
    "\n",
    "        cal_accum_avg(gray_frame, accumulated_weight)\n",
    "\n",
    "        cv2.putText(frame_copy, \"FETCHING BACKGROUND...PLEASE WAIT\",(80, 400), cv2.FONT_HERSHEY_SIMPLEX, 0.9, (0, 0, 255), 2)\n",
    "\n",
    "\n",
    "    else:\n",
    "        hand = segment_hand(gray_frame)#segmenting the hand region\n",
    "        if hand is not None:\n",
    "            thresholded, hand_segment = hand\n",
    "\n",
    "            cv2.drawContours(frame_copy, [hand_segment + (ROI_right,ROI_top)], -1, (255, 0, 0), 1)\n",
    "\n",
    "            cv2.imshow(\"Thesholded Hand Image\", thresholded)\n",
    "            print('1')\n",
    "            thresholded = cv2.resize(thresholded, (64, 64))\n",
    "            thresholded = cv2.cvtColor(thresholded,  cv2.COLOR_GRAY2RGB)\n",
    "            thresholded=thresholded[:,:,0]\n",
    "\n",
    "            x = thresholded\n",
    "            x = np.asarray(thresholded)\n",
    "            x = thresholded.astype('float32')\n",
    "            x /= 255.0\n",
    "            x = np.expand_dims(x, axis=0)\n",
    "            images = np.vstack([x])\n",
    "            print(images.shape)\n",
    "            thresholded = np.reshape(images, (1, 64, 64))\n",
    "            print('2')\n",
    "            pred = model.predict(thresholded)\n",
    "            cv2.putText(frame_copy, word_dict[np.argmax(pred)],(170, 45), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 255), 2)\n",
    "            print(word_dict[np.argmax(pred)])\n",
    "\n",
    "    # Draw ROI on frame_copy\n",
    "    cv2.rectangle(frame_copy, (ROI_left, ROI_top), (ROI_right,  ROI_bottom), (255, 128, 0), 1)\n",
    "    print('yes')\n",
    "\n",
    "    # incrementing the number of frames for tracking\n",
    "    num_frames += 1\n",
    "\n",
    "    # Display the frame with segmented hand\n",
    "    cv2.putText(frame_copy, \"DataFlair hand sign recognition_ _ _\",\n",
    "                (10, 20), cv2.FONT_ITALIC, 0.5, (51, 255, 51), 1)\n",
    "    cv2.imshow(\"Sign Detection\", frame_copy)\n",
    "\n",
    "    # Close windows with Esc\n",
    "    k = cv2.waitKey(1) & 0xFF\n",
    "\n",
    "    if k == 27:\n",
    "        break\n",
    "\n",
    "# Release the camera and destroy all the windows\n",
    "cam.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 295
    },
    "id": "JOj0pSs83sXZ",
    "outputId": "ab6989b9-3955-4a4c-a4f7-5bf9ae061b3f"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.plot(history.history['accuracy'])\n",
    "plt.title(\"Model accuracy\")\n",
    "plt.ylabel(\"accuracy\")\n",
    "plt.xlabel(\"epochs\")\n",
    "plt.ylim(0.8,1)\n",
    "plt.legend([\"train\"],loc = 'upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "z6q-sffe39sh",
    "outputId": "89f58f93-d34a-4141-f317-0ddfb701ab77"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "134/134 [==============================] - 2s 11ms/step - loss: 2.7202 - accuracy: 0.7187\n",
      "Accuracy on test dataset: 0.7186567187309265\n"
     ]
    }
   ],
   "source": [
    "num_test_examples = len(allimg_test)\n",
    "test_loss, test_accuracy = model2.evaluate(test_dataset, steps=math.ceil(num_test_examples/32))\n",
    "print('Accuracy on test dataset:', test_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fxkIV0IZ8CKw",
    "outputId": "43943199-27d0-4968-e151-b43008892a06"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_2\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_6 (InputLayer)        [(None, 64, 64, 3)]       0         \n",
      "                                                                 \n",
      " resnet50 (Functional)       (None, 2, 2, 2048)        23587712  \n",
      "                                                                 \n",
      " global_average_pooling2d_2   (None, 2048)             0         \n",
      " (GlobalAveragePooling2D)                                        \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 150)               307350    \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 32)                4832      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 23,899,894\n",
      "Trainable params: 312,182\n",
      "Non-trainable params: 23,587,712\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Initialize the Pretrained Model\n",
    "feature_extractor = tf.keras.applications.ResNet50(weights='imagenet', \n",
    "                             input_shape=(64, 64, 3),\n",
    "                             include_top=False)\n",
    "\n",
    "# Set this parameter to make sure it's not being trained\n",
    "feature_extractor.trainable = False\n",
    "\n",
    "# Set the input layer\n",
    "input_ = tf.keras.Input(shape=(64, 64, 3))\n",
    "\n",
    "# Set the feature extractor layer\n",
    "x = feature_extractor(input_, training=False)\n",
    "\n",
    "# Set the pooling layer\n",
    "x = tf.keras.layers.GlobalAveragePooling2D()(x)\n",
    "\n",
    "x=tf.keras.layers.Dense(150, activation='relu')(x)\n",
    "\n",
    "output_ = tf.keras.layers.Dense(32, activation=tf.nn.softmax)(x)\n",
    "\n",
    "\n",
    "# Create the new model object\n",
    "model = tf.keras.Model(inputs=input_, outputs=output_)\n",
    "\n",
    "# Compile it\n",
    "model.compile(optimizer='adam',\n",
    "             loss='sparse_categorical_crossentropy',\n",
    "             metrics=['accuracy'])\n",
    "\n",
    "# Print The Summary of The Model\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "15XvlSmP8Hm2",
    "outputId": "a0e248de-db70-4312-b56f-1c7cad9abe81"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "1925/1925 [==============================] - 27s 14ms/step - loss: 0.2877 - accuracy: 0.9115\n",
      "Epoch 2/5\n",
      "1925/1925 [==============================] - 26s 14ms/step - loss: 0.2873 - accuracy: 0.9108\n",
      "Epoch 3/5\n",
      "1925/1925 [==============================] - 27s 14ms/step - loss: 0.2907 - accuracy: 0.9100\n",
      "Epoch 4/5\n",
      "1925/1925 [==============================] - 26s 14ms/step - loss: 0.2803 - accuracy: 0.9127\n",
      "Epoch 5/5\n",
      "1925/1925 [==============================] - 26s 14ms/step - loss: 0.2912 - accuracy: 0.9104\n"
     ]
    }
   ],
   "source": [
    "history1=model.fit(train_dataset, epochs=5, steps_per_epoch=math.ceil(num_train_examples/BATCH_SIZE))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 295
    },
    "id": "tSCRjH258KUG",
    "outputId": "b28be7e0-c172-4421-b5ae-36b3cb1a3922"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.plot(history1.history['accuracy'])\n",
    "plt.title(\"Model accuracy\")\n",
    "plt.ylabel(\"accuracy\")\n",
    "plt.xlabel(\"epochs\")\n",
    "plt.ylim(0.8,1)\n",
    "plt.legend([\"train\"],loc = 'upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "IrOcNVKo8dWx",
    "outputId": "f867ead4-cba5-422a-ad26-d5144565073a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "134/134 [==============================] - 5s 26ms/step - loss: 9.1788 - accuracy: 0.4649\n",
      "Accuracy on test dataset: 0.4649253785610199\n"
     ]
    }
   ],
   "source": [
    "num_test_examples = len(allimg_test)\n",
    "test_loss, test_accuracy = model.evaluate(test_dataset, steps=math.ceil(num_test_examples/32))\n",
    "print('Accuracy on test dataset:', test_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zoi0uRpXsoOK",
    "outputId": "656f185b-8be0-458e-ba32-f79d603b6c9f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_5\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_12 (InputLayer)       [(None, 64, 64, 3)]       0         \n",
      "                                                                 \n",
      " efficientnetb0 (Functional)  (None, 2, 2, 1280)       4049571   \n",
      "                                                                 \n",
      " global_average_pooling2d_5   (None, 1280)             0         \n",
      " (GlobalAveragePooling2D)                                        \n",
      "                                                                 \n",
      " dense_9 (Dense)             (None, 150)               192150    \n",
      "                                                                 \n",
      " dense_10 (Dense)            (None, 32)                4832      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 4,246,553\n",
      "Trainable params: 196,982\n",
      "Non-trainable params: 4,049,571\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Initialize the Pretrained Model\n",
    "feature_extractor = tf.keras.applications.EfficientNetB0(weights='imagenet', \n",
    "                             input_shape=(64, 64, 3),\n",
    "                             include_top=False)\n",
    "\n",
    "# Set this parameter to make sure it's not being trained\n",
    "feature_extractor.trainable = False\n",
    "\n",
    "# Set the input layer\n",
    "input_ = tf.keras.Input(shape=(64, 64, 3))\n",
    "\n",
    "# Set the feature extractor layer\n",
    "x = feature_extractor(input_, training=False)\n",
    "\n",
    "# Set the pooling layer\n",
    "x = tf.keras.layers.GlobalAveragePooling2D()(x)\n",
    "\n",
    "x=tf.keras.layers.Dense(150, activation='relu')(x)\n",
    "\n",
    "output_ = tf.keras.layers.Dense(32, activation=tf.nn.softmax)(x)\n",
    "\n",
    "\n",
    "# Create the new model object\n",
    "model3 = tf.keras.Model(inputs=input_, outputs=output_)\n",
    "\n",
    "# Compile it\n",
    "model3.compile(optimizer='adam',\n",
    "             loss='sparse_categorical_crossentropy',\n",
    "             metrics=['accuracy'])\n",
    "\n",
    "# Print The Summary of The Model\n",
    "model3.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CNJFZZbQs-yc",
    "outputId": "e0f3b590-5276-473e-deab-ed2adfe26ef2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "1925/1925 [==============================] - 59s 31ms/step - loss: 3.4509 - accuracy: 0.0395\n",
      "Epoch 2/5\n",
      "1925/1925 [==============================] - 61s 32ms/step - loss: 3.4588 - accuracy: 0.0394\n",
      "Epoch 3/5\n",
      "1925/1925 [==============================] - 41s 21ms/step - loss: 3.4590 - accuracy: 0.0380\n",
      "Epoch 4/5\n",
      "1925/1925 [==============================] - 22s 11ms/step - loss: 3.4582 - accuracy: 0.0389\n",
      "Epoch 5/5\n",
      "1925/1925 [==============================] - 22s 11ms/step - loss: 3.4589 - accuracy: 0.0403\n"
     ]
    }
   ],
   "source": [
    "history1=model3.fit(train_dataset, epochs=5, steps_per_epoch=math.ceil(num_train_examples/BATCH_SIZE))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yjhJ3W-pfJYY"
   },
   "outputs": [],
   "source": [
    "num_test_examples = len(allimg_test)\n",
    "#from tensorflow.keras.applications import EfficientNetB0\n",
    "model = tf.keras.applications.EfficientNetB0(include_top=False, weights='imagenet', drop_connect_rate=0.4)\n",
    "IMG_SIZE = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 626
    },
    "id": "wQbPglqdfnqe",
    "outputId": "0224abaa-b764-49a4-cb23-6093a8ace1b9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using MirroredStrategy with devices ('/job:localhost/replica:0/task:0/device:GPU:0',)\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "WARNING:tensorflow:Model was constructed with shape (None, 224, 224, 3) for input KerasTensor(type_spec=TensorSpec(shape=(None, 224, 224, 3), dtype=tf.float32, name='input_31'), name='input_31', description=\"created by layer 'input_31'\"), but it was called on an input with incompatible shape (None, 64, 64, 3).\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "ignored",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-46-c7a0a9f77757>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapplications\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEfficientNetB0\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minclude_top\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweights\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclasses\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m     \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m     model.compile(\n\u001b[1;32m     13\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"adam\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"sparse_categorical_crossentropy\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"accuracy\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/training/tracking/base.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    627\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_self_setattr_tracking\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    628\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 629\u001b[0;31m       \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    630\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    631\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_self_setattr_tracking\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprevious_value\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/engine/functional.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, inputs, outputs, name, trainable, **kwargs)\u001b[0m\n\u001b[1;32m    142\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mv1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecuting_eagerly_outside_functions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    143\u001b[0m       if not all([functional_utils.is_input_keras_tensor(t)\n\u001b[0;32m--> 144\u001b[0;31m                   for t in tf.nest.flatten(inputs)]):\n\u001b[0m\u001b[1;32m    145\u001b[0m         \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunctional_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclone_graph_nodes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    146\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_init_graph_network\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/engine/functional.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    142\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mv1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecuting_eagerly_outside_functions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    143\u001b[0m       if not all([functional_utils.is_input_keras_tensor(t)\n\u001b[0;32m--> 144\u001b[0;31m                   for t in tf.nest.flatten(inputs)]):\n\u001b[0m\u001b[1;32m    145\u001b[0m         \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunctional_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclone_graph_nodes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    146\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_init_graph_network\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/engine/functional_utils.py\u001b[0m in \u001b[0;36mis_input_keras_tensor\u001b[0;34m(tensor)\u001b[0m\n\u001b[1;32m     45\u001b[0m   \"\"\"\n\u001b[1;32m     46\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mnode_module\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_keras_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 47\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_KERAS_TENSOR_TYPE_CHECK_ERROR_MSG\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     48\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mtensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnode\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_input\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Found unexpected instance while processing input tensors for keras functional model. Expecting KerasTensor which is from tf.keras.Input() or output from keras layer call(). Got: 64"
     ]
    }
   ],
   "source": [
    "\n",
    "from re import X\n",
    "strategy = tf.distribute.MirroredStrategy()\n",
    "\n",
    "\n",
    "with strategy.scope():\n",
    "    x = tf.keras.layers.Input(shape=(64, 64, 3))\n",
    "    \n",
    "    outputs = tf.keras.applications.EfficientNetB0(include_top=True, weights=None, classes=32)(x)\n",
    "\n",
    "    model = tf.keras.Model(X, outputs)\n",
    "    model.compile(\n",
    "        optimizer=\"adam\", loss=\"sparse_categorical_crossentropy\", metrics=[\"accuracy\"]\n",
    "    )\n",
    "\n",
    "model.summary()\n",
    "\n",
    "epochs = 52  # @param {type: \"slider\", min:10, max:100}\n",
    "hist = model.fit(train_dataset, epochs=epochs, verbose=2)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Sign Language Detection 2.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
