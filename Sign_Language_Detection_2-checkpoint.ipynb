{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Oj_-BCwqSpvx"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import glob\n",
    "import cv2\n",
    "import math\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "AhM8XTWDS1_6",
    "outputId": "f84696c2-a0ab-4171-df2a-c96e1957e108"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xaMtlzZDS5x5"
   },
   "outputs": [],
   "source": [
    "allimg_train = []\n",
    "alllbl_train = []\n",
    "allimg_test = []\n",
    "alllbl_test = []\n",
    "i=0\n",
    "for folder in os.listdir('/content/drive/MyDrive/pcd/pythonProject/dataset'):\n",
    "  imgL=[]\n",
    "  lblL=[]\n",
    "  i=i+1\n",
    "  for img in glob.glob(\"/content/drive/MyDrive/pcd/pythonProject/dataset/\"+folder+\"/*.jpg\"):\n",
    "    n = cv2.imread(img)\n",
    "    n = cv2.resize(n, (64,64), interpolation = cv2.INTER_AREA)\n",
    "    imgL.append(n)\n",
    "    lblL.append(i)\n",
    "  allimg_train.extend(imgL[:int(0.9*len(imgL))])\n",
    "  allimg_test.extend(imgL[int(0.9*len(imgL)):-1])\n",
    "  alllbl_train.extend(lblL[:int(0.9*len(imgL))])\n",
    "  alllbl_test.extend(lblL[int(0.9*len(imgL)):-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "UoXQkSqWg3sz",
    "outputId": "742ffd30-4d13-4155-fa94-a5fb1ffa10f8"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "38492"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(alllbl_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "c0KetUVv0Y2y"
   },
   "outputs": [],
   "source": [
    "for i in range(len(alllbl_train)):\n",
    "  alllbl_train[i]=alllbl_train[i]-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ad05Ll3CrVp_"
   },
   "outputs": [],
   "source": [
    "for i in range(len(alllbl_test)):\n",
    "  alllbl_test[i]=alllbl_test[i]-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nao3tWz8esUs"
   },
   "outputs": [],
   "source": [
    "allimg_train = tf.constant(allimg_train)\n",
    "alllbl_train = tf.constant(alllbl_train)\n",
    "allimg_test = tf.constant(allimg_test)\n",
    "alllbl_test = tf.constant(alllbl_test)\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((allimg_train , alllbl_train))\n",
    "test_dataset = tf.data.Dataset.from_tensor_slices((allimg_test, alllbl_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "db4dbO8cbCQe",
    "outputId": "ba291762-1abc-4317-9b69-f6e5dcc33099"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4261"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(allimg_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "75i-1TOCWHlY"
   },
   "outputs": [],
   "source": [
    "def normalize(images, labels):\n",
    "  \n",
    "  images = tf.cast(images, tf.float32)\n",
    "  images /= 255\n",
    "  return images, labels\n",
    "\n",
    "\n",
    "train_dataset =  train_dataset.map(normalize)\n",
    "test_dataset  =  test_dataset.map(normalize)\n",
    "\n",
    "\n",
    "train_dataset =  train_dataset.cache()\n",
    "test_dataset  =  test_dataset.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RFVvPICkWdA4"
   },
   "outputs": [],
   "source": [
    "BATCH_SIZE = 20\n",
    "num_train_examples = len(allimg_train)\n",
    "train_dataset = train_dataset.cache().repeat().shuffle(num_train_examples).batch(BATCH_SIZE)\n",
    "test_dataset = test_dataset.cache().batch(BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "f6HSTJQ8Wfqf",
    "outputId": "cdfb3598-4de5-4980-8196-1a81e9dcbfc8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d (Conv2D)             (None, 64, 64, 128)       3584      \n",
      "                                                                 \n",
      " max_pooling2d (MaxPooling2D  (None, 21, 21, 128)      0         \n",
      " )                                                               \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 21, 21, 128)       0         \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 21, 21, 128)       147584    \n",
      "                                                                 \n",
      " max_pooling2d_1 (MaxPooling  (None, 7, 7, 128)        0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " conv2d_2 (Conv2D)           (None, 7, 7, 64)          73792     \n",
      "                                                                 \n",
      " max_pooling2d_2 (MaxPooling  (None, 2, 2, 64)         0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " conv2d_3 (Conv2D)           (None, 2, 2, 64)          16448     \n",
      "                                                                 \n",
      " max_pooling2d_3 (MaxPooling  (None, 1, 1, 64)         0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 1, 1, 64)          0         \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 64)                0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 256)               16640     \n",
      "                                                                 \n",
      " dropout_2 (Dropout)         (None, 256)               0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 32)                8224      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 266,272\n",
      "Trainable params: 266,272\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model2 = tf.keras.Sequential()\n",
    "model2.add(tf.keras.layers.Conv2D(filters=128, kernel_size=3, padding='same', activation='relu', input_shape=(64,64,3))) \n",
    "model2.add(tf.keras.layers.MaxPooling2D(pool_size=3))\n",
    "model2.add(tf.keras.layers.Dropout(0.3))\n",
    "model2.add(tf.keras.layers.Conv2D(filters=128, kernel_size=3, padding='same', activation='relu'))\n",
    "model2.add(tf.keras.layers.MaxPooling2D(pool_size=3))\n",
    "model2.add(tf.keras.layers.Conv2D(filters=64, kernel_size=3, padding='same', activation='relu'))\n",
    "model2.add(tf.keras.layers.MaxPooling2D(pool_size=3))\n",
    "model2.add(tf.keras.layers.Conv2D(filters=64, kernel_size=2, padding='same', activation='relu'))\n",
    "model2.add(tf.keras.layers.MaxPooling2D(pool_size=2))\n",
    "model2.add(tf.keras.layers.Dropout(0.3))\n",
    "model2.add(tf.keras.layers.Flatten())\n",
    "model2.add(tf.keras.layers.Dense(256, activation='relu'))\n",
    "model2.add(tf.keras.layers.Dropout(0.5))                \n",
    "model2.add(tf.keras.layers.Dense(32, activation='softmax'))\n",
    "\n",
    "model2.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PjDfl9TxWusQ"
   },
   "outputs": [],
   "source": [
    "model2.compile(optimizer='adam',\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KnkRRWAsCrat"
   },
   "outputs": [],
   "source": [
    "from keras.callbacks import ReduceLROnPlateau\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=1, min_lr=0.0005)\n",
    "early_stop = EarlyStopping(monitor='val_loss', min_delta=0, patience=2, verbose=0, mode='auto')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "B0DHNBLhW1Af",
    "outputId": "acf0725a-74fe-40c9-ec08-43d9ac0eb27c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "3849/3850 [============================>.] - ETA: 0s - loss: 0.2441 - accuracy: 0.9322WARNING:tensorflow:Learning rate reduction is conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy,lr\n",
      "WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy,lr\n",
      "3850/3850 [==============================] - 46s 12ms/step - loss: 0.2441 - accuracy: 0.9323 - lr: 0.0010\n",
      "Epoch 2/10\n",
      "3848/3850 [============================>.] - ETA: 0s - loss: 0.2464 - accuracy: 0.9331WARNING:tensorflow:Learning rate reduction is conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy,lr\n",
      "WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy,lr\n",
      "3850/3850 [==============================] - 45s 12ms/step - loss: 0.2463 - accuracy: 0.9331 - lr: 0.0010\n",
      "Epoch 3/10\n",
      "3847/3850 [============================>.] - ETA: 0s - loss: 0.2390 - accuracy: 0.9349WARNING:tensorflow:Learning rate reduction is conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy,lr\n",
      "WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy,lr\n",
      "3850/3850 [==============================] - 44s 11ms/step - loss: 0.2389 - accuracy: 0.9350 - lr: 0.0010\n",
      "Epoch 4/10\n",
      "3847/3850 [============================>.] - ETA: 0s - loss: 0.2505 - accuracy: 0.9319WARNING:tensorflow:Learning rate reduction is conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy,lr\n",
      "WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy,lr\n",
      "3850/3850 [==============================] - 44s 11ms/step - loss: 0.2504 - accuracy: 0.9319 - lr: 0.0010\n",
      "Epoch 5/10\n",
      "3848/3850 [============================>.] - ETA: 0s - loss: 0.2356 - accuracy: 0.9365WARNING:tensorflow:Learning rate reduction is conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy,lr\n",
      "WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy,lr\n",
      "3850/3850 [==============================] - 44s 11ms/step - loss: 0.2356 - accuracy: 0.9365 - lr: 0.0010\n",
      "Epoch 6/10\n",
      "3848/3850 [============================>.] - ETA: 0s - loss: 0.2471 - accuracy: 0.9341WARNING:tensorflow:Learning rate reduction is conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy,lr\n",
      "WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy,lr\n",
      "3850/3850 [==============================] - 44s 11ms/step - loss: 0.2471 - accuracy: 0.9341 - lr: 0.0010\n",
      "Epoch 7/10\n",
      "3850/3850 [==============================] - ETA: 0s - loss: 0.2389 - accuracy: 0.9371WARNING:tensorflow:Learning rate reduction is conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy,lr\n",
      "WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy,lr\n",
      "3850/3850 [==============================] - 44s 11ms/step - loss: 0.2389 - accuracy: 0.9371 - lr: 0.0010\n",
      "Epoch 8/10\n",
      "3846/3850 [============================>.] - ETA: 0s - loss: 0.2469 - accuracy: 0.9353WARNING:tensorflow:Learning rate reduction is conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy,lr\n",
      "WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy,lr\n",
      "3850/3850 [==============================] - 44s 11ms/step - loss: 0.2468 - accuracy: 0.9353 - lr: 0.0010\n",
      "Epoch 9/10\n",
      "3846/3850 [============================>.] - ETA: 0s - loss: 0.2511 - accuracy: 0.9338WARNING:tensorflow:Learning rate reduction is conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy,lr\n",
      "WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy,lr\n",
      "3850/3850 [==============================] - 44s 11ms/step - loss: 0.2509 - accuracy: 0.9338 - lr: 0.0010\n",
      "Epoch 10/10\n",
      "3850/3850 [==============================] - ETA: 0s - loss: 0.2485 - accuracy: 0.9381WARNING:tensorflow:Learning rate reduction is conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy,lr\n",
      "WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy,lr\n",
      "3850/3850 [==============================] - 44s 11ms/step - loss: 0.2485 - accuracy: 0.9381 - lr: 0.0010\n"
     ]
    }
   ],
   "source": [
    "\n",
    "history=model2.fit(train_dataset, epochs=10, steps_per_epoch=math.ceil(num_train_examples/BATCH_SIZE),callbacks=[reduce_lr, early_stop])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ASu0CUM-i3-H",
    "outputId": "87964cf7-43ee-4224-a53a-1840f7359c1b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /content/drive/MyDrive/pcd/pythonProject/alphabetcha1.model/assets\n"
     ]
    }
   ],
   "source": [
    " model2.save('/content/drive/MyDrive/pcd/pythonProject/alphabetcha1.model')"
   ]
  }
  
